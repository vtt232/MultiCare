using Distributions

using JuMP
using GLPK
using Ipopt
using DataStructures
using Random

function Base.findmax(f::Function, xs)
    f_max = -Inf
    x_max = first(xs)
    for x in xs
        v = f(x)
        if v > f_max
            f_max, x_max = v, x
        end
    end
    return f_max, x_max
end

Base.argmax(f::Function, xs) = findmax(f, xs)[2]
    
Base.Dict{Symbol,V}(a::NamedTuple) where V =
    Dict{Symbol,V}(n=>v for (n,v) in zip(keys(a), values(a)))


struct DiscountFactor
    y::Float64

    function DiscountFactor()
        new(0.9)
    end
end

struct Agent
    a::Vector{Int64}

    function Agent()
        x=[1,2]
        new(x)
    end
end

struct StateSpace
    stateSpace::Vector{String}

    function StateSpace()
        x=["hungry","sated"]
        new(x)
        
    end
end

struct ActionSpace
    actionSpace::Vector{String}

    function ActionSpace()
        x=["feed","sing","ignore"]
        new(x)
    end
end

struct ObservationSpace
    observationSpace::Vector{String}

    function ObservationSpace()
        x=["crying","quiet"]
        new(x)
    end
end 

function Transition(fromState, actions::Tuple{String,String}, toState )
    if fromState=="hungry" && toState=="sated" && (actions[1]=="feed" || actions[2]=="feed") 
        return 1;
    elseif fromState=="hungry" && toState=="sated" && (actions[1]!="feed" && actions[2]!="feed")
        return 0.5;
    elseif fromState=="hungry" && toState=="hungry"&& (actions[1]!="feed" && actions[2]!="feed")
        return 1;
    elseif fromState=="hungry" && toState=="hungry"&& (actions[1]=="feed" || actions[2]=="feed")
        return 0;
    elseif fromState=="sated" && toState=="sated" && (actions[1]=="feed" || actions[2]=="feed")
        return 1;
    elseif fromState=="sated" && toState=="sated" && (actions[1]!="feed" && actions[2]!="feed")
        return 0.9;
    elseif fromState=="sated" && toState=="hungry" && (actions[1]!="feed" && actions[2]!="feed")
        return 0.1;
    elseif fromState=="sated" && toState=="hungry" && (actions[1]=="feed" || actions[2]=="feed")
        return 0;
    else return 0;
    end
end

#Transition("hungry",("sing","feed"),"hungry")
    
function Observation(actions::Tuple{String,String},toState::String,observes::Tuple{String, String})
    if (observes[1]=="crying"&&observes[2]=="crying") && (actions[1]=="sing"||actions[2]=="sing")&&toState=="hungry"
        return 0.9;
    elseif (observes[1]=="quiet"&&observes[2]=="quiet") && (actions[1]=="sing"||actions[2]=="sing")&&toState=="hungry"
        return 0.1;
    elseif (observes[1]=="crying"&&observes[2]=="crying") && (actions[1]=="sing"||actions[2]=="sing")&&toState=="sated"
        return 0;
    elseif (observes[1]=="quiet"&&observes[2]=="quiet") && (actions[1]=="sing"||actions[2]=="sing")&&toState=="sated"
        return 1;
    elseif (observes[1]=="crying"&&observes[2]=="crying") && (actions[1]!="sing"&&actions[2]!="sing")&&toState=="hungry"
        return 0.9;
    elseif (observes[1]=="quiet"&&observes[2]=="quiet") && (actions[1]!="sing"&&actions[2]!="sing")&&toState=="hungry"
        return 0.1;
    elseif (observes[1]=="crying"&&observes[2]=="crying") && (actions[1]!="sing"&&actions[2]!="sing")&&toState=="sated"
        return 0;
    elseif (observes[1]=="quiet"&&observes[2]=="quiet") && (actions[1]!="sing"&&actions[2]!="sing")&&toState=="sated"
        return 1;
    else return 0;
    end
end

function Reward(state,actions::Tuple{String,String})
    r1=0.0;
    r2=0.0;
    if state=="hungry"
        r1=r1-10.0;
        r2=r2-10.0;
    end
    if actions[1]=="feed"
        r1=r1-2.5;
    end
    if actions[2]=="feed"
        r2=r2-5.0;
    end
    if actions[1]=="sing"
        r1=r1-0.5;
    end
    if actions[2]=="sing"
        r2=r2-0.25;
    end

    return [r1,r2];
end

#r=Reward("hungry",("ignore","ignore"))




struct POMG
    Î³ # discount factor
    â„# agents
    ğ’®# state space
    ğ’œ # joint action space
    ğ’ª # joint observation space
    T # transition function
    O # joint observation function
    R # joint reward function
    function POMG(discount,agents,states,jointAction,jointObservation,transitionFunction,jointObservationFunction,jointRewardFunction)
        new(discount,agents,states,jointAction,jointObservation,transitionFunction,jointObservationFunction,jointRewardFunction);
    end
end

#m=POMG();
#print(m);

struct ConditionalPlan
    a # action to take at root
    subplans # dictionary mapping observations to subplans
end
    ConditionalPlan(a) = ConditionalPlan(a, Dict())
    (Ï€::ConditionalPlan)() = Ï€.a
    (Ï€::ConditionalPlan)(o) = Ï€.subplans[o]




#const policyAgent1= Dict(
#    "quiet"=>ConditionalPlan("sing"), "crying"=>ConditionalPlan("ignore")
#)
#ConditionalPlan("ignore",policyAgent1)


joint(X) = vec(collect(Iterators.product(X...)))


function lookahead(ğ’«::POMG, U, s, a)
    ğ’®, ğ’ª, T, O, R, Î³ = ğ’«.ğ’®, joint(ğ’«.ğ’ª), ğ’«.T, ğ’«.O, ğ’«.R, ğ’«.Î³
    uâ€² = sum(T(s,a,sâ€²)*sum(O(a,sâ€²,o)*U(o,sâ€²) for o in ğ’ª) for sâ€² in ğ’®)
    return R(s,a) + Î³*uâ€²
end

function evaluate_plan(ğ’«::POMG, Ï€, s)
    a = Tuple(Ï€i() for Ï€i in Ï€)
    U(o,sâ€²) = evaluate_plan(ğ’«, [Ï€i(oi) for (Ï€i, oi) in zip(Ï€,o)], sâ€²)
    return isempty(first(Ï€).subplans) ? ğ’«.R(s,a) : lookahead(ğ’«, U, s, a)
end
function utility(ğ’«::POMG, b, Ï€)
    u = [evaluate_plan(ğ’«, Ï€, s) for s in ğ’«.ğ’®]
    return sum(bs * us for (bs, us) in zip(b, u))
end

p=POMG(0.9,[1,2],["hungry","sated"],[["feed","sing","ignore"],["feed","sing","ignore"]],[["crying","quiet"],["crying","quiet"]],Transition,Observation,Reward);
b=[0.5,0.5];
Ï€1= Dict(
    "crying"=>ConditionalPlan("feed"), "quiet"=>ConditionalPlan("ignore")
);
Ï€2= Dict(
    "quiet"=>ConditionalPlan("feed"), "crying"=>ConditionalPlan("sing")
);

Ï€s= [ConditionalPlan("ignore",Ï€1),ConditionalPlan("ignore",Ï€2)];
#ConditionalPlan("ignore",policyAgent1)
#print(first(Ï€s).subplans)
utility(p,b,Ï€s)
#-------------------------------------------

#---------------------------------------
struct SimpleGame
    Î³     # discount factor
    â„     # agents
    ğ’œ     # joint action space
    R       # joint reward funct
    function SimpleGame(discount, agents, jointActionSpace, jointRewardFunc)
      new(discount, agents, jointActionSpace, jointRewardFunc)
    end
end

struct SimpleGamePolicy
    p # dictionary mapping actions to probabilities
    function SimpleGamePolicy(p::Base.Generator)
        return SimpleGamePolicy(Dict(p))
    end
    function SimpleGamePolicy(p::Dict)
        vs = collect(values(p))
        vs ./= sum(vs)
    return new(Dict(k => v for (k,v) in zip(keys(p), vs)))
    end
    SimpleGamePolicy(ai) = new(Dict(ai => 1.0))
end
(Ï€i::SimpleGamePolicy)(ai) = get(Ï€i.p, ai, 0.0)

    

struct NashEquilibrium end
function tensorform(ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    â„â€² = eachindex(â„)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(a) for a in joint(ğ’œ)]
    return â„â€², ğ’œâ€², Râ€²
end
  
function solve(M::NashEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = tensorform(ğ’«)
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„])
    @variable(model, Ï€[i=â„, ğ’œ[i]] â‰¥ 0)
    @NLobjective(model, Min,
      sum(U[i] - sum(prod(Ï€[j,a[j]] for j in â„) * R[y][i]
        for (y,a) in enumerate(joint(ğ’œ))) for i in â„))
    @NLconstraint(model, [i=â„, ai=ğ’œ[i]],
      U[i] â‰¥ sum(
        prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„)
        * R[y][i] for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„], sum(Ï€[i,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€iâ€²(i) = SimpleGamePolicy(ğ’«.ğ’œ[i][ai] => value(Ï€[i,ai]) for ai in ğ’œ[i])
    return [Ï€iâ€²(i) for i in â„]
end

struct POMGNashEquilibrium
    b # initial belief
    d # depth of conditional plans
end
function create_conditional_plans(ğ’«, d)
    â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    for t in 1:d
        Î  = expand_conditional_plans(ğ’«, Î )
    end
    return Î 
end
function expand_conditional_plans(ğ’«, Î )
    â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
    return [[ConditionalPlan(ai, Dict(oi => Ï€i for oi in ğ’ª[i]))
        for Ï€i in Î [i] for ai in ğ’œ[i]] for i in â„]
end
function solve(M::POMGNashEquilibrium, ğ’«::POMG)
    â„, Î³, b, d = ğ’«.â„, ğ’«.Î³, M.b, M.d
    Î  = create_conditional_plans(ğ’«, d)
    U = Dict(Ï€ => utility(ğ’«, b, Ï€) for Ï€ in joint(Î ))
    ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> U[Ï€])
    Ï€ = solve(NashEquilibrium(), ğ’¢)
    return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end

#p=POMG(0.9,[1,2],["hungry","sated"],[["feed","sing","ignore"],["feed","sing","ignore"]],[["crying","quiet"],["crying","quiet"]],Transition,Observation,Reward);
#b=[0.5,0.5];
#dyP=POMGNashEquilibrium(b,2);
#print(solve(dyP,p));

#----------------------------
struct SimpleGame
    Î³     # discount factor
    â„     # agents
    ğ’œ     # joint action space
    R       # joint reward funct
    function SimpleGame(discount, agents, jointActionSpace, jointRewardFunc)
      new(discount, agents, jointActionSpace, jointRewardFunc)
    end
  end




struct NashEquilibrium end
function tensorform(ğ’«::SimpleGame)
    â„, ğ’œ, R = ğ’«.â„, ğ’«.ğ’œ, ğ’«.R
    â„â€² = eachindex(â„)
    ğ’œâ€² = [eachindex(ğ’œ[i]) for i in â„]
    Râ€² = [R(a) for a in joint(ğ’œ)]
    return â„â€², ğ’œâ€², Râ€²
end
  
function solve(M::NashEquilibrium, ğ’«::SimpleGame)
    â„, ğ’œ, R = tensorform(ğ’«)
    model = Model(Ipopt.Optimizer)
    @variable(model, U[â„])
    @variable(model, Ï€[i=â„, ğ’œ[i]] â‰¥ 0)
    @NLobjective(model, Min,
      sum(U[i] - sum(prod(Ï€[j,a[j]] for j in â„) * R[y][i]
        for (y,a) in enumerate(joint(ğ’œ))) for i in â„))
    @NLconstraint(model, [i=â„, ai=ğ’œ[i]],
      U[i] â‰¥ sum(
        prod(j==i ? (a[j]==ai ? 1.0 : 0.0) : Ï€[j,a[j]] for j in â„)
        * R[y][i] for (y,a) in enumerate(joint(ğ’œ))))
    @constraint(model, [i=â„], sum(Ï€[i,ai] for ai in ğ’œ[i]) == 1)
    optimize!(model)
    Ï€iâ€²(i) = SimpleGamePolicy(ğ’«.ğ’œ[i][ai] => value(Ï€[i,ai]) for ai in ğ’œ[i])
    return [Ï€iâ€²(i) for i in â„]
end
  

function expand_conditional_plans(ğ’«, Î )
    â„, ğ’œ, ğ’ª = ğ’«.â„, ğ’«.ğ’œ, ğ’«.ğ’ª
    return [[ConditionalPlan(ai, Dict(oi => Ï€i for oi in ğ’ª[i]))
        for Ï€i in Î [i] for ai in ğ’œ[i]] for i in â„]
end


struct POMGDynamicProgramming
    b # initial belief
    d # depth of conditional plans
end
function solve(M::POMGDynamicProgramming, ğ’«::POMG)
    â„, ğ’®, ğ’œ, R, Î³, b, d = ğ’«.â„, ğ’«.ğ’®, ğ’«.ğ’œ, ğ’«.R, ğ’«.Î³, M.b, M.d
    Î  = [[ConditionalPlan(ai) for ai in ğ’œ[i]] for i in â„]
    for t in 1:d
        Î  = expand_conditional_plans(ğ’«, Î )
        prune_dominated!(Î , ğ’«)
    end
    ğ’¢ = SimpleGame(Î³, â„, Î , Ï€ -> utility(ğ’«, b, Ï€))
    Ï€ = solve(NashEquilibrium(), ğ’¢)
    return Tuple(argmax(Ï€i.p) for Ï€i in Ï€)
end

function prune_dominated!(Î , ğ’«::POMG)
    done = false
    while !done
        done = true
        for i in shuffle(ğ’«.â„)
            for Ï€i in shuffle(Î [i])
                if length(Î [i]) > 1 && is_dominated(ğ’«, Î , i, Ï€i)
                    filter!(Ï€iâ€² -> Ï€iâ€² â‰  Ï€i, Î [i])
                    done = false
                    break
                end
            end
        end
    end
end

function is_dominated(ğ’«::POMG, Î , i, Ï€i)
    â„, ğ’® = ğ’«.â„, ğ’«.ğ’®
    jointÎ noti = joint([Î [j] for j in â„ if j â‰  i])
    Ï€(Ï€iâ€², Ï€noti) = [j==i ? Ï€iâ€² : Ï€noti[j>i ? j-1 : j] for j in â„]
    Ui = Dict((Ï€iâ€², Ï€noti, s) => evaluate_plan(ğ’«, Ï€(Ï€iâ€², Ï€noti), s)[i]
            for Ï€iâ€² in Î [i], Ï€noti in jointÎ noti, s in ğ’®)
    model = Model(Ipopt.Optimizer)
    @variable(model, Î´)
    @variable(model, b[jointÎ noti, ğ’®] â‰¥ 0)
    @objective(model, Max, Î´)
    @constraint(model, [Ï€iâ€²=Î [i]],
        sum(b[Ï€noti, s] * (Ui[Ï€iâ€², Ï€noti, s] - Ui[Ï€i, Ï€noti, s])
        for Ï€noti in jointÎ noti for s in ğ’®) â‰¥ Î´)
    @constraint(model, sum(b) == 1)
    optimize!(model)
    return value(Î´) â‰¥ 0
end
   

p=POMG(0.9,[1,2],["hungry","sated"],[["feed","sing","ignore"],["feed","sing","ignore"]],[["crying","quiet"],["crying","quiet"]],Transition,Observation,Reward);
b=[0.5,0.5];
dyP=POMGDynamicProgramming(b,2);
print(solve(dyP,p));










